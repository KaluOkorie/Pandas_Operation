# -*- coding: utf-8 -*-
"""Pandas.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W1iVXMV9Fw8OMXdYW2CK7MPm8uy1Cg9w

# Analyzing Educational Performance Patterns Using Basic Pandas Operations

## Library Imports and Data Loading
###Pandas Operations
* info()
* describe()
* Counting all Null Values
* Count of Unique Values
* Sorting on Values
* Correlation between Columns
* Heatmap using Seaborn
* Grouping Data
* Visualizing Grouped Data
* Average scores
* Boxplots for Outliers
* Data Types
* Columns Renames
* Key insight

## About Project
- This project explores student performance trends across math, reading, and writing, using real dataset insights.
- The goal is to help educational institutions better understand learning gaps, allocate resources more effectively, and design targeted interventions.
- It also demonstrates how to use core pandas operations for data cleaning, exploration, grouping, aggregation, and basic statistical analysis.
"""

# Import Required Libraries
import pandas as pd           # DataFrame creation and manipulation
import numpy as np            # Numerical operations and array handling
import seaborn as sns          # Statistical data visualization
import matplotlib.pyplot as plt    # Plotting and charting tools

# Data Loading
df=pd.read_csv('StudentsPerformance.csv') # to import the dataset
df.head(4) # Display the first 4 rows of the dataset

"""## Operation 1: info()
* To Understand dataset structure and memory usage
"""

df.info()

print("""Dataset Info
----------------------
Rows: 1000 students
Columns: 8 total
Categorical: gender, race/ethnicity, parental education, lunch, test prep
Numeric: math, reading, writing scores
Data Quality: No missing values (all 1000 rows complete)
Memory Usage: 62.6 KB (lightweight, easy to process)
""")

"""##Operation 2: describe()
- To get statistical summary of numerical columns
"""

df.describe()

print ("""
Statistical Summary Insights:
1. Overall Performance Level:
   - Mean scores indicate overall performance.
   - Reading has the highest average, while Math is the lowest.

2. Score Variability:
   - Standard deviations show the spread of scores.
   - No subject shows unusually high or low performance variation.

3. High Performers:
   - The 75th percentile is around 77–79 in all subjects.

4. Minimum Scores:
   - Math: 0.  - Reading: 17. - Writing: 10

5. Overall Takeaway:
   - Students tend to perform best in reading,
     closely followed by writing, and slightly lower in math.
""")

"""## Operation 3: Counting all Null Values
* Assess data quality and completeness
"""

print("---Missing Values Analysis---")
df.isnull().sum()

"""## Operation 4: Count of Unique Values
* Intended: Understand categorical variable distribution
"""

print("---Unique Values Analysis---")
for col in df.select_dtypes(include='object').columns:   # Loop through each categorical (object type) column
    values = df[col].unique()   # Get all unique values in the column
    count = len(values)          # Count how many unique values there are
    print(col, ":", count, "unique values")      # Print results in a simple way
    print("Values:")
    for v in values:
        print(" -", v)
    print()                  # blank line for readability

assumptions = """
---Assumption---
- Dataset is binary: female/male. Clean and well-structured.
- Groups represent categorical clusters of ethnicity in the study/school dataset.
- Some high school parents attended high school but did not graduate.
  Some college  parents attended college but did not complete a degree.
- Indicates socio-economic differences: standard vs free/reduced lunch.
"""

print(assumptions)

"""# Operation 5: Sorting on Values
* To identify top/bottom performers
"""

# Top 5 math scores
top5_math = df.sort_values(by='math score', ascending=False).head(5)
top5_math

"""## Operation 6: Correlation between Columns
*  Find relationships between numerical variables
"""

print("--- Correlation Matrix ---")
numeric_cols = df.select_dtypes(include=[np.number]).columns
correlation_matrix = df[numeric_cols].corr()
print(correlation_matrix)

correlation_result = ('''-----correlation_result-------
* Academic performance across subjects is highly related, particularly between reading and writing.
* Math is slightly less correlated with language-based subjects, but still strongly positive.
''')
print(correlation_result)

"""## Operation 7: Heatmap using Seaborn
* Visualize correlation patterns clearly
"""

print("--- Generating Correlation Heatmap ---")
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Score Correlations Heatmap")
plt.show()

"""## Operation 8: Grouping Data
* Comparing average performance across categories
"""

print("--- Performance by Parental Education Level ---\n")

grouped_data = df.groupby("parental level of education").agg({
    "math score": ["mean", "count", "std"],
    "reading score": "mean",
    "writing score": "mean"
}).round(2)

# Flatten MultiIndex columns into single stringsfor plotting
grouped_data.columns = [
    "_".join(col) if isinstance(col, tuple) else col
    for col in grouped_data.columns
]

print(grouped_data)

"""## Operation 9: Visualizing Grouped Data
* Make grouped comparisons visually compelling
"""

grouped_data[["math score_mean", "reading score_mean", "writing score_mean"]].plot(
    kind="bar", figsize=(6,4)
)

plt.title("Average Scores by Parental Education Level")
plt.ylabel("Average Score")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

observation = (
    "---- OBSERVATION ------\n"
    "Higher parental education correlates strongly with better student performance \n"
    "in all three subjects. Students whose parents hold master’s or bachelor’s degrees \n"
    "achieve the highest average scores, while those with parents who completed only \n"
    "high school or some high school have noticeably lower outcomes. \n"
)

print(observation)

#Plot 2

# Calculate average scores by gender
gender_avg = df.groupby("gender")[["math score", "reading score", "writing score"]].mean()

# Plot as a bar chart
gender_avg.plot(kind="bar", color=["#FF6B6B", "#4ECDC4", "#45B7D1"], figsize=(8,5))

# Add labels and title
plt.title("Average Scores by Gender", fontsize=14)
plt.ylabel("Average Score")
plt.xticks(rotation=0)  # keep gender labels horizontal
plt.tight_layout()
plt.show()

# Calculate average scores by lunch type
lunch_avg = df.groupby("lunch")[["math score", "reading score", "writing score"]].mean()

# Plot as a bar chart
lunch_avg.plot(kind="bar", color=["#95E1D3", "#F38181", "#FFD93D"], figsize=(8,5))

# Add labels and title
plt.title("Average Scores by Lunch Type", fontsize=14)
plt.ylabel("Average Score")
plt.xticks(rotation=0)  # keep lunch labels horizontal
plt.tight_layout()
plt.show()

"""## Operation 10: Boxplots for Outliers
* Identify outliers in score distributions
"""

print("\n--- Outlier Detection with Boxplots ---")

# Choose the scores
scores = ["math score", "reading score", "writing score"]

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(8, 5))

# Loop through each score and plot
for idx, score in enumerate(scores):
    sns.boxplot(y=df[score], ax=axes[idx], color=["#FF6B6B", "#4ECDC4", "#45B7D1"][idx])
    axes[idx].set_title(f"{score.title()} Distribution", fontsize=12)
    axes[idx].set_ylabel("Score")

plt.suptitle("Outlier Detection in Student Scores", fontsize=14)
plt.tight_layout()
plt.show()

Observation = ('''\n----- Observation -----
Between Reading, and Writing, the score distributions look very similar:
- Most students score between 55 and 80.
- All subjects show several low outliers, meaning some students performed significantly below average.
''')
print (Observation )

"""## Operation 11: Data Types
* Verify data types for appropriate analysis
"""

print(df.dtypes)
print(f"Total categorical columns: {len(df.select_dtypes(include=['object']).columns)}")
print(f"Total numerical columns: {len(df.select_dtypes(include=[np.number]).columns)}")

"""## Operation 12: Columns Renames
* Standardize column names for consistency
"""

print("\n--- Column Renaming ----")

# Create a mapping dictionary
rename_dict = {
    "math score": "math_score",
    "reading score": "reading_score",
    "writing score": "writing_score",
    "parental level of education": "parental_education",
    "test preparation course": "test_prep",
    "race/ethnicity": "ethnicity"
}

# Apply renaming
df = df.rename(columns=rename_dict)

# Check the new column names
df.head(5)

"""## KEY INSIGHTS"""

def generate_comprehensive_insights(df):
    print(" COMPREHENSIVE INSIGHTS")

    # Create summary metrics
    df["total_score"] = df["math_score"] + df["reading_score"] + df["writing_score"]
    df["average_score"] = df["total_score"] / 3

    # Insight 1: Overall Performance Distribution
    performance_categories = pd.cut(
        df["average_score"],
        bins=[0, 60, 70, 80, 90, 100],
        labels=["Needs Help", "Below Avg", "Average", "Good", "Excellent"]
    )

    print("\n INSIGHT 1: Performance Distribution")
    print("-" * 40)
    perf_dist = performance_categories.value_counts(normalize=True) * 100
    for category, percentage in perf_dist.items():
        print(f"  {category}: {percentage:.1f}% of students")

        # Insight 2: Gender Performance Gap
    gender_performance = df.groupby('gender')['average_score'].agg(['mean', 'std'])
    print("\n INSIGHT 2: Gender Performance Gap")
    print("-" * 40)
    print(f"  Female Avg Score: {gender_performance.loc['female', 'mean']:.1f}")
    print(f"  Male Avg Score:   {gender_performance.loc['male', 'mean']:.1f}")
    print(f"  Difference:       {abs(gender_performance.loc['female', 'mean'] - gender_performance.loc['male', 'mean']):.1f} points")

     # Insight 3: ROI of Test Preparation
    test_prep_impact = df.groupby('test_prep')['average_score'].mean()
    print("\n INSIGHT 3: Test Preparation ROI")
    print("-" * 40)
    impact = test_prep_impact['completed'] - test_prep_impact['none']
    print(f"  Test Prep Impact: +{impact:.1f} points average improvement")
    print(f"  ROI Calculation: For every student completing test prep,")
    print(f"                   expected improvement: {impact:.1f} points")

    # Insight 4: Socioeconomic Factors
    lunch_impact = df.groupby('lunch')['average_score'].mean()
    print("\n INSIGHT 4: Socioeconomic Impact")
    print("-" * 40)
    print(f"  Standard Lunch Avg:   {lunch_impact['standard']:.1f}")
    print(f"  Free/Reduced Avg:     {lunch_impact['free/reduced']:.1f}")
    print(f"  Achievement Gap:      {lunch_impact['standard'] - lunch_impact['free/reduced']:.1f} points")

    # Insight 5: Parental Education Tier Analysis
    edu_tiers = {
        'high': ["master's degree", "bachelor's degree"],
        'medium': ["associate's degree", "some college"],
        'low': ["high school", "some high school"]
    }

    print("\n INSIGHT 5: Parental Education Tiers")
    print("-" * 40)
    for tier, levels in edu_tiers.items():
        tier_avg = df[df['parental_education'].isin(levels)]['average_score'].mean()
        print(f"  {tier.title()} Tier: {tier_avg:.1f}")

    # Insight 6: At-Risk Student Identification
    at_risk = df[
        (df['math_score'] < 60) |
        (df['reading_score'] < 60) |
        (df['writing_score'] < 60)
    ]

    print("\n INSIGHT 6: At-Risk Student Profile")
    print("-" * 40)
    print(f"  Total At-Risk Students: {len(at_risk)} ({len(at_risk)/len(df)*100:.1f}%)")

    # Profile of at-risk students
    if len(at_risk) > 0:
        print(f"  Common Characteristics:")
        print(f"    • {at_risk['test_prep'].value_counts(normalize=True).get('none', 0)*100:.1f}% had no test prep")
        print(f"    • {at_risk['lunch'].value_counts(normalize=True).get('free/reduced', 0)*100:.1f}% have free/reduced lunch")
        print(f"    • Most common parental education: {at_risk['parental_education'].mode().iloc[0]}")


generate_comprehensive_insights(df)

Summary = '''--- Summary ---

This analysis reveals that behind every statistic are real students facing real challenges.
The data shows us that nearly 40% need additional support, but more importantly,
it provides clear paths to help them—from targeted test preparation to addressing basic needs through lunch programs.\n

What stands out is how manageable solutions can create meaningful change.
A 7.6-point improvement from test prep isn't just a number; it's the difference between struggling and succeeding for hundreds of students.
The achievement gaps we've identified aren't inevitable—they're addressable through thoughtful, data-informed interventions.\n

Ultimately, this project demonstrates that when we listen to what the data tells us about student experiences,
we can build more supportive and equitable learning environments.
The true value lies in using these insights to make practical decisions that help real students thrive.'''

print(Summary)

